<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Griffin【简单使用】</title>
    <url>/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/</url>
    <content><![CDATA[<h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><h3 id="建库"><a href="#建库" class="headerlink" title="建库"></a>建库</h3><figure class="highlight ada"><table><tr><td class="code"><pre><span class="line">hive -e <span class="string">"create database griffin_demo"</span></span><br><span class="line">hive <span class="comment">--database griffin_demo</span></span><br></pre></td></tr></table></figure>
<h3 id="建表"><a href="#建表" class="headerlink" title="建表"></a>建表</h3><figure class="highlight n1ql"><table><tr><td class="code"><pre><span class="line">hive&gt; <span class="keyword">CREATE</span> EXTERNAL TABLE <span class="symbol">`demo_src`</span>(</span><br><span class="line">    &gt;   <span class="symbol">`id`</span> bigint,</span><br><span class="line">    &gt;   <span class="symbol">`age`</span> int,</span><br><span class="line">    &gt;   <span class="symbol">`desc`</span> <span class="keyword">string</span>)</span><br><span class="line">    &gt; PARTITIONED <span class="keyword">BY</span> (</span><br><span class="line">    &gt;   <span class="symbol">`dt`</span> <span class="keyword">string</span>,</span><br><span class="line">    &gt;   <span class="symbol">`hour`</span> <span class="keyword">string</span>)</span><br><span class="line">    &gt; ROW FORMAT DELIMITED</span><br><span class="line">    &gt;   FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">'|'</span></span><br><span class="line">    &gt; LOCATION</span><br><span class="line">    &gt;   <span class="string">'hdfs:///griffin/data/batch/demo_src'</span>;</span><br><span class="line"></span><br><span class="line">hive&gt; <span class="keyword">CREATE</span> EXTERNAL TABLE <span class="symbol">`demo_tgt`</span>(</span><br><span class="line">    &gt;   <span class="symbol">`id`</span> bigint,</span><br><span class="line">    &gt;   <span class="symbol">`age`</span> int,</span><br><span class="line">    &gt;   <span class="symbol">`desc`</span> <span class="keyword">string</span>)</span><br><span class="line">    &gt; PARTITIONED <span class="keyword">BY</span> (</span><br><span class="line">    &gt;   <span class="symbol">`dt`</span> <span class="keyword">string</span>,</span><br><span class="line">    &gt;   <span class="symbol">`hour`</span> <span class="keyword">string</span>)</span><br><span class="line">    &gt; ROW FORMAT DELIMITED</span><br><span class="line">    &gt;   FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">'|'</span></span><br><span class="line">    &gt; LOCATION</span><br><span class="line">	&gt;   <span class="string">'hdfs:///griffin/data/batch/demo_tgt'</span>;</span><br></pre></td></tr></table></figure>
<h3 id="load数据"><a href="#load数据" class="headerlink" title="load数据"></a>load数据</h3><p>(为了更直观的验证计算结果的准确性在此不按照官网上的操作进行)<br>（1）数据<br>demo_src.txt(10条)</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="number">1000</span>|<span class="number">10</span>|a</span><br><span class="line"><span class="number">1001</span>|<span class="number">11</span>|</span><br><span class="line"><span class="number">1002</span>|<span class="number">16</span>|b</span><br><span class="line"><span class="number">1003</span>|<span class="number">21</span>|</span><br><span class="line"><span class="number">1004</span>|<span class="number">10</span>|c</span><br><span class="line"><span class="number">1005</span>|<span class="number">11</span>|</span><br><span class="line"><span class="number">1006</span>|<span class="number">16</span>|d</span><br><span class="line"><span class="number">1007</span>|<span class="number">21</span>|</span><br><span class="line"><span class="number">1008</span>|<span class="number">10</span>|e</span><br><span class="line"><span class="number">1009</span>|<span class="number">18</span>|</span><br></pre></td></tr></table></figure>
<p>demo_tgt.txt(10条)</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="number">1001</span>|<span class="number">10</span>|a</span><br><span class="line"><span class="number">1002</span>|<span class="number">10</span>|</span><br><span class="line"><span class="number">1003</span>|<span class="number">16</span>|b</span><br><span class="line"><span class="number">1004</span>|<span class="number">10</span>|</span><br><span class="line"><span class="number">1005</span>|<span class="number">16</span>|c</span><br><span class="line"><span class="number">1006</span>|<span class="number">10</span>|</span><br><span class="line"><span class="number">1007</span>|<span class="number">16</span>|d</span><br><span class="line"><span class="number">1008</span>|<span class="number">10</span>|</span><br><span class="line"><span class="number">1009</span>|<span class="number">16</span>|e</span><br><span class="line"><span class="number">1010</span>|<span class="number">10</span>|</span><br></pre></td></tr></table></figure>
<p>（2）load数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/griffin/data/demo_src.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> demo_src <span class="keyword">partition</span>(dt=<span class="number">20200117</span>,<span class="keyword">hour</span>=<span class="number">9</span>)</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/griffin/data/demo_tgt.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> demo_tgt <span class="keyword">partition</span>(dt=<span class="number">20200117</span>,<span class="keyword">hour</span>=<span class="number">9</span>)</span><br></pre></td></tr></table></figure>
<h2 id="UI操作"><a href="#UI操作" class="headerlink" title="UI操作"></a>UI操作</h2><p>UI操作文档连接：<a href="https://github.com/apache/griffin/blob/master/griffin-doc/ui/user-guide.md" target="_blank" rel="noopener">https://github.com/apache/griffin/blob/master/griffin-doc/ui/user-guide.md</a></p>
<h3 id="程序流程图"><a href="#程序流程图" class="headerlink" title="程序流程图"></a>程序流程图</h3><img src="/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/01.png" class="" title="This is an image">
<h3 id="Accuracy操作"><a href="#Accuracy操作" class="headerlink" title="Accuracy操作"></a>Accuracy操作</h3><p>Create Measure<br>（1）单击Measures，然后选择Create Measure。您可以使用该度量来处理数据并获得所需的结果。</p>
<img src="/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/02.png" class="" title="This is an image">
<p>主要有四种选择供您选择：<br>如果要测量源与目标之间的匹配率，请选择准确性。<br>如果要检查数据的特定值（例如：空列数），请选择分析。</p>
<img src="/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/03.png" class="" title="This is an image">
<p>（2）选择Accuracy<br>（3）选择将用于比较的源数据集和字段。</p>
<img src="/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/04.png" class="" title="This is an image">
<p>（4）选择将用于比较的目标数据集和字段。</p>
<img src="/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/05.png" class="" title="This is an image">
<p>（5）映射源数据集和目标<br>选择匹配源和目标的规则。这里有6个选项可供选择：<br>=：两列的数据应完全匹配。<br>！=：两列的数据应该不同。</p>
<blockquote>
<p>：目标列数据应大于源列数据。<br>=：目标列数据应大于或等于源列数据。<br>&lt;：目标列数据应小于源列数据。<br>&lt;=：目标列数据应小于或等于源列数据。</p>
</blockquote>
<img src="/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/06.png" class="" title="This is an image">
<p>（6）分区配置<br>设置源数据集和目标数据集的分区配置。<br>分区大小是指配置单元数据库的最小数据单位，用于拆分要计算的数据完成的文件路径表示完成的文件路径的格式。注意：该处没有加where条件，关于加不加where条件的效果后面会有总结</p>
<img src="/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/07.png" class="" title="This is an image">
<img src="/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/08.png" class="" title="This is an image">
<img src="/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/09.png" class="" title="This is an image">
<p>假设源表demo_src具有1000条记录，目标表demo_tgt仅具有999条记录，并且demo_tgt与demo_src的Id完全匹配，则准确率= 999/1000 * 100％= 99.9％。</p>
<img src="/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/10.png" class="" title="This is an image">
<p>Create Job</p>
<img src="/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/11.png" class="" title="This is an image">
<img src="/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/12.png" class="" title="This is an image">
<img src="/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/13.png" class="" title="This is an image">
<img src="/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/14.png" class="" title="This is an image">
<p>查看结果</p>
<img src="/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/15.png" class="" title="This is an image">
<img src="/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/16.png" class="" title="This is an image">

<h3 id="Data-Profiling"><a href="#Data-Profiling" class="headerlink" title="Data Profiling"></a>Data Profiling</h3><p>Create Measure</p>
<img src="/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/17.png" class="" title="This is an image">
<p>选择Data Profiling</p>
<img src="/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/18.png" class="" title="This is an image">
<p>选择要统计的表和字段</p>
<img src="/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/19.png" class="" title="This is an image">
<p>简单的统计：空数、去重求count、    count(1)、最大、最低</p>
<img src="/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/20.png" class="" title="This is an image">
<img src="/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/21.png" class="" title="This is an image">
<img src="/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/22.png" class="" title="This is an image">
<img src="/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/23.png" class="" title="This is an image">
<img src="/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/24.png" class="" title="This is an image">
<p>Create Job</p>
<img src="/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/25.png" class="" title="This is an image">
<p>每五分钟计算一次</p>
<img src="/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/26.png" class="" title="This is an image">
<img src="/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/27.png" class="" title="This is an image">
<p>查看结果</p>
<img src="/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/28.png" class="" title="This is an image">
<img src="/2020/01/21/Griffin%E3%80%90%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E3%80%91/29.png" class="" title="This is an image">
<h3 id="操作补充"><a href="#操作补充" class="headerlink" title="操作补充"></a>操作补充</h3><p>创建measure过程中,在Partition Configuration时where的设置问题存在以下三种情况：<br><font size = 2 color = red><br>（说明：分区表分区字段 dt、hour,partition Size=1 hour）<br></font><br><strong>情况一</strong>：where为空，计算库中所有的数据<br>如果where条件是null,不考虑range的范围，计算所有的数据，显示的计算时间为当前时间减range的下限<br>例：表中数据为10条，hour=8 的有6条 hour=9 的有4条，当前时间为17点，range:-3——0 显示的结果：time=14:00:00 count=10</p>
<p><strong>情况二</strong>：where指定具体分区，只计算指定分区的数据<br>如果where的条件为具体的日期，不考虑range的范围，计算指定日期的数据，显示的时间为当前时间减range的下限<br>例：表中数据为10条，hour=8 的有6条 hour=9 的有4条，当前时间为17点，where hour=8 range:-3——0 显示的结果：time=14:00:00 count=6</p>
<p><strong>情况三</strong>：where中的值为dt=#yyyyMMdd# AND hour=#HH#只会计算range范围内的数据<br>如果where中的条件为“dt=#yyyyMMdd# AND hour=#HH#”，会考虑range，计算的数据为range内的数据，显示的时间为当前时间减range的下限<br>例：表中数据为10条，hour=8 的有6条 hour=9 的有4条，当前时间为11点，range:-3——-1 显示的结果：time=11:00:00 count=10，如果range:0——0 这是计算当前分区的数据，显示的时间也为当前时间</p>
]]></content>
      <categories>
        <category>Griffin</category>
      </categories>
  </entry>
  <entry>
    <title>Griffin【安装部署】</title>
    <url>/2020/01/21/Griffin%E3%80%90%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%E3%80%91/</url>
    <content><![CDATA[<p>（官网提供有中文版资料：<a href="http://griffin.apache.org/docs/quickstart-cn.html）" target="_blank" rel="noopener">http://griffin.apache.org/docs/quickstart-cn.html）</a></p>
<h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>JDK (version 1.8)<br>MySQL(version 5.6)<br>Hadoop (version 2.7.2)<br>Hive (version 1.2.1)<br>Spark (version 2.2.1)<br>Livy（version 0.5）<br>ElasticSearch (version 6.3.1)</p>
<h2 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h2><p>在MySQL中创建数据库quartz，然后执行<a href="https://github.com/apache/griffin/blob/master/service/src/main/resources/Init_quartz_mysql_innodb.sql" target="_blank" rel="noopener">Init_quartz_mysql_innodb.sql</a>脚本。</p>
<h2 id="Hadoop和Hive"><a href="#Hadoop和Hive" class="headerlink" title="Hadoop和Hive"></a>Hadoop和Hive</h2><figure class="highlight arduino"><table><tr><td class="code"><pre><span class="line">#创建/<span class="built_in">home</span>/spark_conf目录</span><br><span class="line">hadoop fs -<span class="built_in">mkdir</span> -p /<span class="built_in">home</span>/spark_conf</span><br><span class="line">#上传hive-site.xml</span><br><span class="line">hadoop fs -<span class="built_in">put</span> hive-site.xml /<span class="built_in">home</span>/spark_conf/</span><br><span class="line">启动元数据服务</span><br><span class="line">/opt/<span class="keyword">module</span>/hive/bin/hive --service metastore</span><br></pre></td></tr></table></figure>
<h2 id="设置环境变量"><a href="#设置环境变量" class="headerlink" title="设置环境变量"></a>设置环境变量</h2><p>vim /etc/profile</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">JAVA_HOME</span>=/opt/module/jdk1.8</span><br><span class="line"><span class="comment">#spark目录</span></span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">SPARK_HOME</span>=/opt/module/spark</span><br><span class="line"><span class="comment">#livy命令目录</span></span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">LIVY_HOME</span>=/opt/module/livy/bin</span><br><span class="line"><span class="comment">#hadoop配置文件目录</span></span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">HADOOP_CONF_DIR</span>=/opt/module/hadoop-2.7.2/etc/hadoop</span><br></pre></td></tr></table></figure>
<p>source /etc/profile</p>
<h2 id="Elasticsearch配置"><a href="#Elasticsearch配置" class="headerlink" title="Elasticsearch配置"></a>Elasticsearch配置</h2><p>在ES里创建griffin索引：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">curl -k -H "Content-Type: application/json" -XPUT http://hadoop200:9200/griffin -d '</span><br><span class="line">&#123;</span><br><span class="line">    <span class="attr">"aliases"</span>: &#123;&#125;,</span><br><span class="line">    <span class="attr">"mappings"</span>: &#123;</span><br><span class="line">        <span class="attr">"accuracy"</span>: &#123;</span><br><span class="line">            <span class="attr">"properties"</span>: &#123;</span><br><span class="line">                <span class="attr">"name"</span>: &#123;</span><br><span class="line">                    <span class="attr">"fields"</span>: &#123;</span><br><span class="line">                        <span class="attr">"keyword"</span>: &#123;</span><br><span class="line">                            <span class="attr">"ignore_above"</span>: <span class="number">256</span>,</span><br><span class="line">                            <span class="attr">"type"</span>: <span class="string">"keyword"</span></span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;,</span><br><span class="line">                    <span class="attr">"type"</span>: <span class="string">"text"</span></span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="attr">"tmst"</span>: &#123;</span><br><span class="line">                    <span class="attr">"type"</span>: <span class="string">"date"</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"settings"</span>: &#123;</span><br><span class="line">        <span class="attr">"index"</span>: &#123;</span><br><span class="line">            <span class="attr">"number_of_replicas"</span>: <span class="string">"2"</span>,</span><br><span class="line">            <span class="attr">"number_of_shards"</span>: <span class="string">"5"</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">'</span><br></pre></td></tr></table></figure>
<p>创建成功后会返回<br>{“acknowledged”:true,”shards_acknowledged”:true,”index”:”griffin”}</p>
<h2 id="Griffin源码打包部署"><a href="#Griffin源码打包部署" class="headerlink" title="Griffin源码打包部署"></a>Griffin源码打包部署</h2><p>(1)、下载Griffin源码包<br>Griffin的源码地址是：<a href="https://github.com/apache/griffin.git" target="_blank" rel="noopener">https://github.com/apache/griffin.git</a><br>(2)、使用IDEA打开源码<br>下载完成在idea中导入并展开源码的结构图如下：</p>
<img src="/2020/01/21/Griffin%E3%80%90%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%E3%80%91/01.png" class="" title="This is an image">
<p>Griffin的源码主要包括griffin-doc、measure、service和ui四个模块<br>①    griffin-doc负责存放Griffin的文档;<br>②    measure负责与spark交互，执行统计任务;<br>③ service使用spring boot作为服务实现，负责给ui模块提供交互所需的restful api，保存统计任务，展示统计结果。<br>(3)、修改代码中的配置<br>①service/src/main/resources/application.properties：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Apache Griffin应用名称</span></span><br><span class="line"><span class="meta">spring.application.name</span>=<span class="string">griffin_test</span></span><br><span class="line"><span class="comment"># MySQL数据库配置信息</span></span><br><span class="line"><span class="meta">spring.datasource.url</span>=<span class="string">jdbc:mysql://hadoop200:3306/quartz?useSSL=false</span></span><br><span class="line"><span class="meta">spring.datasource.username</span>=<span class="string">root</span></span><br><span class="line"><span class="meta">spring.datasource.password</span>=<span class="string">Deepexi.123456</span></span><br><span class="line"><span class="meta">spring.jpa.generate-ddl</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">spring.datasource.driver-class-name</span>=<span class="string">com.mysql.jdbc.Driver</span></span><br><span class="line"><span class="meta">spring.jpa.show-sql</span>=<span class="string">true</span></span><br><span class="line"><span class="comment"># Hive metastore</span></span><br><span class="line"><span class="meta">hive.metastore.uris</span>=<span class="string">thrift://hadoop200:9083</span></span><br><span class="line"><span class="meta">hive.metastore.dbname</span>=<span class="string">default</span></span><br><span class="line"><span class="meta">hive.hmshandler.retry.attempts</span>=<span class="string">15</span></span><br><span class="line"><span class="meta">hive.hmshandler.retry.interval</span>=<span class="string">2000ms</span></span><br><span class="line"><span class="comment">#Hive jdbc</span></span><br><span class="line"><span class="meta">hive.jdbc.className</span>=<span class="string">org.apache.hive.jdbc.HiveDriver</span></span><br><span class="line"><span class="meta">hive.jdbc.url</span>=<span class="string">jdbc:hive2://hadoop200</span></span><br><span class="line"><span class="meta">hive.need.kerberos</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">hive.keytab.user</span>=<span class="string">xxx@xx.com</span></span><br><span class="line"><span class="meta">hive.keytab.path</span>=<span class="string">/path/to/keytab/file</span></span><br><span class="line"><span class="comment"># Hive cache time</span></span><br><span class="line"><span class="meta">cache.evict.hive.fixedRate.in.milliseconds</span>=<span class="string">900000</span></span><br><span class="line"><span class="comment"># Kafka schema registry</span></span><br><span class="line"><span class="meta">kafka.schema.registry.url</span>=<span class="string">http:/hadoop200:8081</span></span><br><span class="line"><span class="comment"># Update job instance state at regular intervals</span></span><br><span class="line"><span class="meta">jobInstance.fixedDelay.in.milliseconds</span>=<span class="string">60000</span></span><br><span class="line"><span class="comment"># Expired time of job instance which is 7 days that is 604800000 milliseconds.Time unit only supports milliseconds</span></span><br><span class="line"><span class="meta">jobInstance.expired.milliseconds</span>=<span class="string">604800000</span></span><br><span class="line"><span class="comment"># schedule predicate job every 5 minutes and repeat 12 times at most</span></span><br><span class="line"><span class="comment">#interval time unit s:second m:minute h:hour d:day,only support these four units</span></span><br><span class="line"><span class="meta">predicate.job.interval</span>=<span class="string">5m</span></span><br><span class="line"><span class="meta">predicate.job.repeat.count</span>=<span class="string">12</span></span><br><span class="line"><span class="comment"># external properties directory location</span></span><br><span class="line"><span class="meta">external.config.location</span>=<span class="string"></span></span><br><span class="line"><span class="comment"># external BATCH or STREAMING env</span></span><br><span class="line"><span class="meta">external.env.location</span>=<span class="string"></span></span><br><span class="line"><span class="comment"># login strategy ("default" or "ldap")</span></span><br><span class="line"><span class="meta">login.strategy</span>=<span class="string">default</span></span><br><span class="line"><span class="comment"># ldap</span></span><br><span class="line"><span class="meta">ldap.url</span>=<span class="string">ldap://hostname:port</span></span><br><span class="line"><span class="meta">ldap.email</span>=<span class="string">@example.com</span></span><br><span class="line"><span class="meta">ldap.searchBase</span>=<span class="string">DC=org,DC=example</span></span><br><span class="line"><span class="meta">ldap.searchPattern</span>=<span class="string">(sAMAccountName=&#123;0&#125;)</span></span><br><span class="line"><span class="comment"># hdfs default name</span></span><br><span class="line"><span class="meta">fs.defaultFS</span>=<span class="string"></span></span><br><span class="line"><span class="comment"># elasticsearch</span></span><br><span class="line"><span class="meta">elasticsearch.host</span>=<span class="string">hadoop200</span></span><br><span class="line"><span class="meta">elasticsearch.port</span>=<span class="string">9200</span></span><br><span class="line"><span class="meta">elasticsearch.scheme</span>=<span class="string">http</span></span><br><span class="line"><span class="comment"># elasticsearch.user = user</span></span><br><span class="line"><span class="comment"># elasticsearch.password = password</span></span><br><span class="line"><span class="comment"># livy</span></span><br><span class="line"><span class="meta">livy.uri</span>=<span class="string">http://hadoop200:8998/batches</span></span><br><span class="line"><span class="meta">livy.need.queue</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">livy.task.max.concurrent.count</span>=<span class="string">20</span></span><br><span class="line"><span class="meta">livy.task.submit.interval.second</span>=<span class="string">3</span></span><br><span class="line"><span class="meta">livy.task.appId.retry.count</span>=<span class="string">3</span></span><br><span class="line"><span class="meta">livy.need.kerberos</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">livy.server.auth.kerberos.principal</span>=<span class="string">livy/kerberos.principal</span></span><br><span class="line"><span class="meta">livy.server.auth.kerberos.keytab</span>=<span class="string">/path/to/livy/keytab/file</span></span><br><span class="line"><span class="comment"># yarn url</span></span><br><span class="line"><span class="meta">yarn.uri</span>=<span class="string">http://hadoop200:8088</span></span><br><span class="line"><span class="comment"># griffin event listener</span></span><br><span class="line"><span class="meta">internal.event.listeners</span>=<span class="string">GriffinJobEventHook</span></span><br><span class="line"><span class="comment"># 日志文件</span></span><br><span class="line"><span class="meta">logging.file</span>=<span class="string">logs/griffin-service.log</span></span><br></pre></td></tr></table></figure>
<p>②service/src/main/resources/quartz.properties</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">org.quartz.scheduler.instanceName</span>=<span class="string">spring-boot-quartz</span></span><br><span class="line"><span class="meta">org.quartz.scheduler.instanceId</span>=<span class="string">AUTO</span></span><br><span class="line"><span class="meta">org.quartz.threadPool.threadCount</span>=<span class="string">5</span></span><br><span class="line"><span class="meta">org.quartz.jobStore.class</span>=<span class="string">org.quartz.impl.jdbcjobstore.JobStoreTX</span></span><br><span class="line"><span class="comment"># If you use mysql as your database,set this property value to org.quartz.impl.jdbcjobstore.StdJDBCDelegate</span></span><br><span class="line"><span class="meta">org.quartz.jobStore.driverDelegateClass</span>=<span class="string">org.quartz.impl.jdbcjobstore.StdJDBCDelegate</span></span><br><span class="line"><span class="meta">org.quartz.jobStore.useProperties</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">org.quartz.jobStore.misfireThreshold</span>=<span class="string">60000</span></span><br><span class="line"><span class="meta">org.quartz.jobStore.tablePrefix</span>=<span class="string">QRTZ_</span></span><br><span class="line"><span class="meta">org.quartz.jobStore.isClustered</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">org.quartz.jobStore.clusterCheckinInterval</span>=<span class="string">20000</span></span><br></pre></td></tr></table></figure>
<p>③service/src/main/resources/sparkProperties.json</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"file"</span>: <span class="string">"hdfs:///griffin/griffin-measure.jar"</span>,</span><br><span class="line">  <span class="attr">"className"</span>: <span class="string">"org.apache.griffin.measure.Application"</span>,</span><br><span class="line">  <span class="attr">"queue"</span>: <span class="string">"default"</span>,</span><br><span class="line">  <span class="attr">"numExecutors"</span>: <span class="number">2</span>,</span><br><span class="line">  <span class="attr">"executorCores"</span>: <span class="number">1</span>,</span><br><span class="line">  <span class="attr">"driverMemory"</span>: <span class="string">"1g"</span>,</span><br><span class="line">  <span class="attr">"executorMemory"</span>: <span class="string">"1g"</span>,</span><br><span class="line">  <span class="attr">"conf"</span>: &#123;</span><br><span class="line">    <span class="attr">"spark.yarn.dist.files"</span>: <span class="string">"hdfs:///home/spark_conf/hive-site.xml"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"files"</span>: [</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>④service/src/main/resources/env/env_batch.json</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"spark"</span>: &#123;</span><br><span class="line">    <span class="attr">"log.level"</span>: <span class="string">"WARN"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"sinks"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">"type"</span>: <span class="string">"CONSOLE"</span>,</span><br><span class="line">      <span class="attr">"config"</span>: &#123;</span><br><span class="line">        <span class="attr">"max.log.lines"</span>: <span class="number">10</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">"type"</span>: <span class="string">"HDFS"</span>,</span><br><span class="line">      <span class="attr">"config"</span>: &#123;</span><br><span class="line">        <span class="attr">"path"</span>: <span class="string">"hdfs:///griffin/persist"</span>,</span><br><span class="line">        <span class="attr">"max.persist.lines"</span>: <span class="number">10000</span>,</span><br><span class="line">        <span class="attr">"max.lines.per.file"</span>: <span class="number">10000</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">"type"</span>: <span class="string">"ELASTICSEARCH"</span>,</span><br><span class="line">      <span class="attr">"config"</span>: &#123;</span><br><span class="line">        <span class="attr">"method"</span>: <span class="string">"post"</span>,</span><br><span class="line">        <span class="attr">"api"</span>: <span class="string">"http://hadoop200:9200/griffin/accuracy"</span>,</span><br><span class="line">        <span class="attr">"connection.timeout"</span>: <span class="string">"1m"</span>,</span><br><span class="line">        <span class="attr">"retry"</span>: <span class="number">10</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="attr">"griffin.checkpoint"</span>: []</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>⑤service/src/main/resources/env/env_streaming.json</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"spark"</span>: &#123;</span><br><span class="line">    <span class="attr">"log.level"</span>: <span class="string">"WARN"</span>,</span><br><span class="line">    <span class="attr">"checkpoint.dir"</span>: <span class="string">"hdfs:///griffin/checkpoint/$&#123;JOB_NAME&#125;"</span>,</span><br><span class="line">    <span class="attr">"init.clear"</span>: <span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"batch.interval"</span>: <span class="string">"1m"</span>,</span><br><span class="line">    <span class="attr">"process.interval"</span>: <span class="string">"5m"</span>,</span><br><span class="line">    <span class="attr">"config"</span>: &#123;</span><br><span class="line">      <span class="attr">"spark.default.parallelism"</span>: <span class="number">4</span>,</span><br><span class="line">      <span class="attr">"spark.task.maxFailures"</span>: <span class="number">5</span>,</span><br><span class="line">      <span class="attr">"spark.streaming.kafkaMaxRatePerPartition"</span>: <span class="number">1000</span>,</span><br><span class="line">      <span class="attr">"spark.streaming.concurrentJobs"</span>: <span class="number">4</span>,</span><br><span class="line">      <span class="attr">"spark.yarn.maxAppAttempts"</span>: <span class="number">5</span>,</span><br><span class="line">      <span class="attr">"spark.yarn.am.attemptFailuresValidityInterval"</span>: <span class="string">"1h"</span>,</span><br><span class="line">      <span class="attr">"spark.yarn.max.executor.failures"</span>: <span class="number">120</span>,</span><br><span class="line">      <span class="attr">"spark.yarn.executor.failuresValidityInterval"</span>: <span class="string">"1h"</span>,</span><br><span class="line">      <span class="attr">"spark.hadoop.fs.hdfs.impl.disable.cache"</span>: <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"sinks"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">"type"</span>: <span class="string">"CONSOLE"</span>,</span><br><span class="line">      <span class="attr">"config"</span>: &#123;</span><br><span class="line">        <span class="attr">"max.log.lines"</span>: <span class="number">100</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">"type"</span>: <span class="string">"HDFS"</span>,</span><br><span class="line">      <span class="attr">"config"</span>: &#123;</span><br><span class="line">        <span class="attr">"path"</span>: <span class="string">"hdfs:///griffin/persist"</span>,</span><br><span class="line">        <span class="attr">"max.persist.lines"</span>: <span class="number">10000</span>,</span><br><span class="line">        <span class="attr">"max.lines.per.file"</span>: <span class="number">10000</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">"type"</span>: <span class="string">"ELASTICSEARCH"</span>,</span><br><span class="line">      <span class="attr">"config"</span>: &#123;</span><br><span class="line">        <span class="attr">"method"</span>: <span class="string">"post"</span>,</span><br><span class="line">        <span class="attr">"api"</span>: <span class="string">"http://hadoop200:9200/griffin/accuracy"</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="attr">"griffin.checkpoint"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">"type"</span>: <span class="string">"zk"</span>,</span><br><span class="line">      <span class="attr">"config"</span>: &#123;</span><br><span class="line">        <span class="attr">"hosts"</span>: <span class="string">"hadoop200:2181, hadoop201:2181, hadoop202:2181"</span>,</span><br><span class="line">        <span class="attr">"namespace"</span>: <span class="string">"griffin/infocache"</span>,</span><br><span class="line">        <span class="attr">"lock.path"</span>: <span class="string">"lock"</span>,</span><br><span class="line">        <span class="attr">"mode"</span>: <span class="string">"persist"</span>,</span><br><span class="line">        <span class="attr">"init.clear"</span>: <span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"close.clear"</span>: <span class="literal">false</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>(4)打包<br>mvn -Dmaven.test.skip=true clean install</p>
<img src="/2020/01/21/Griffin%E3%80%90%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%E3%80%91/02.png" class="" title="This is an image">

<p><strong>编译打包时可能遇到的问题</strong><br>问题：IDEA中使用maven命令进行编译打包时报错，报错信息如下：</p>
<img src="/2020/01/21/Griffin%E3%80%90%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%E3%80%91/03.png" class="" title="This is an image">
<p>原因：在UI在的pom.xml文件中，默认指定了安装node的版本号。如果之前本地没有安装过node的话，应该不会报这个错误的。因为我之前安装过的，所以需要修改pom.xml里面的node版本号：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">nodeVersion</span>&gt;</span>$&#123;node.version&#125;<span class="tag">&lt;/<span class="name">nodeVersion</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">npmVersion</span>&gt;</span>$&#123;npm.version&#125;<span class="tag">&lt;/<span class="name">npmVersion</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>需要改成对应本地的版本号。</p>
<h2 id="上传jar包并启动"><a href="#上传jar包并启动" class="headerlink" title="上传jar包并启动"></a>上传jar包并启动</h2><p>打包命令执行完成后，会在service和measure模块的target目录下分别看到service-0.4.0.jar和measure-0.4.0.jar两个jar，将这两个jar分别拷贝到服务器对应的目录下。<br>1、    创建文件夹<br>mkdir /opt/module/griffin<br>2、    将service-0.4.0.jar和measure-0.4.0.jar上传至/opt/module/griffin目录下<br>3、    将measure-0.4.0.jar这个jar上传到HDFS的/griffin文件目录里, 因为spark在yarn集群上执行任务时，需要到HDFS的/griffin目录下加载griffin-measure.jar</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment">#修改jar名称</span></span><br><span class="line"><span class="attr">mv</span> <span class="string">measure-0.4.0.jar griffin-measure.jar</span></span><br><span class="line"><span class="comment">#在HDFS中创建griffin目录</span></span><br><span class="line"><span class="attr">hadoop</span> <span class="string">fs -mkdir /griffin</span></span><br><span class="line"><span class="comment">#上传griffin-measure.jar到HDFS文件目录里</span></span><br><span class="line"><span class="attr">hadoop</span> <span class="string">fs -put measure-0.4.0.jar /griffin/</span></span><br></pre></td></tr></table></figure>
<p>4、    运行service-0.4.0.jar，启动Griffin管理后台：<br>nohup java -jar service-0.4.0.jar&gt;service.out 2&gt;&amp;1 &amp;</p>
<h2 id="访问Griffin页面"><a href="#访问Griffin页面" class="headerlink" title="访问Griffin页面"></a>访问Griffin页面</h2><p>浏览器打开<a href="http://hadoop200:8080" target="_blank" rel="noopener">http://hadoop200:8080</a> 用户名：admmin密码：admin</p>
<img src="/2020/01/21/Griffin%E3%80%90%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%E3%80%91/04.png" class="" title="This is an image">]]></content>
      <categories>
        <category>Griffin</category>
      </categories>
  </entry>
  <entry>
    <title>Elasticsearch安装教程</title>
    <url>/2020/01/19/Elasticsearch%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<font size = 2 color = red >
备注：全程使用cwq用户操作，因为elasticsearch不允许直接使用root启动
</font>

<h2 id="单机版"><a href="#单机版" class="headerlink" title="单机版"></a>单机版</h2><p>1.下载地址<br>&emsp;官网：<a href="https://www.elastic.co/cn/downloads/elasticsearch" target="_blank" rel="noopener">https://www.elastic.co/cn/downloads/elasticsearch</a></p>
<img src="/2020/01/19/Elasticsearch%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/01.png" class="" title="This is an image">

<p>2.上传文件<br>&emsp;通过远程连接工具将文件上传到 /opt/software/目录下</p>
<p>3.解压<br>&emsp;将上传的tar包解压到opt/module/目录下<br>&emsp;tar-zxvf elasticsearch-6.3.1.tar.gz -C /opt/module/</p>
<p>4.修改配置<br>&emsp;修改elasticsearch-6.3.1/config/elasticsearch.yml文件中的host和port</p>
<img src="/2020/01/19/Elasticsearch%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/02.png" class="" title="This is an image">

<p>5.启动ES<br>&emsp;进入解压后es的目录下然后执行启动命令<br>&emsp;[cwq@hadoop200 elasticsearch-6.3.1]# bin/elasticsearch</p>
<p>6.测试<br>&emsp;<a href="http://hadoop200:9200" target="_blank" rel="noopener">http://hadoop200:9200</a></p>
<img src="/2020/01/19/Elasticsearch%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/03.png" class="" title="This is an image">

<h2 id="集群版"><a href="#集群版" class="headerlink" title="集群版"></a>集群版</h2><p>步骤 1: 下载安装包<br>&emsp;官网：<a href="https://www.elastic.co/cn/downloads/elasticsearch" target="_blank" rel="noopener">https://www.elastic.co/cn/downloads/elasticsearch</a></p>
<p>步骤 2: 解压<br>&emsp;tar -zxvf elasticsearch-6.3.1.tar.gz -C /opt/module</p>
<p>步骤 3: 修改配置文件config/elasticsearch.yml</p>
<figure class="highlight vala"><table><tr><td class="code"><pre><span class="line"><span class="meta">#---------------------------------- Cluster ----------------------------------</span></span><br><span class="line"><span class="meta"># 设置集群名称</span></span><br><span class="line">cluster.name: myES</span><br><span class="line"><span class="meta"># ------------------------------------ Node ------------------------------------</span></span><br><span class="line"><span class="meta"># 当前节点名称，各个节点是不一样的</span></span><br><span class="line">node.name: node<span class="number">-200</span></span><br><span class="line"><span class="meta"># ----------------------------------- Memory -----------------------------------</span></span><br><span class="line"><span class="meta"># 关闭bootstrap的自检程序</span></span><br><span class="line">bootstrap.memory_lock: <span class="literal">false</span></span><br><span class="line">bootstrap.system_call_filter: <span class="literal">false</span></span><br><span class="line"><span class="meta"># ---------------------------------- Network -----------------------------------</span></span><br><span class="line"><span class="meta"># 给当前节点绑定 ip 地址</span></span><br><span class="line">network.host: hadoop200</span><br><span class="line"><span class="meta"># 端口号保持默认 9200</span></span><br><span class="line"><span class="meta">#http.port: 9200</span></span><br><span class="line"><span class="meta"># --------------------------------- Discovery ----------------------------------</span></span><br><span class="line"><span class="meta"># 集群个节点IP地址，也可以使用域名，需要各节点能够解析</span></span><br><span class="line">discovery.zen.ping.unicast.hosts: [<span class="string">"hadoop200"</span>, <span class="string">"hadoop201"</span>, <span class="string">"hadoop202"</span>]</span><br></pre></td></tr></table></figure>

<p>步骤 4: 分发 ElasticSearch<br>&emsp;注意修改每个节点的名</p>
<img src="/2020/01/19/Elasticsearch%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/04.png" class="" title="This is an image">

<p>步骤 5: 启动 ElasticSearch<br>&emsp;分别在 3 台设备上启动 ElasticSearch<br>&emsp;bin/elasticsearch &amp;</p>
<p>步骤 6: 查看 ElasticSearch 是否启动成功</p>
<img src="/2020/01/19/Elasticsearch%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/05.png" class="" title="This is an image">

<p>步骤 7: 测试是否可以连接到 ElasticSearch<br>&emsp;curl <a href="http://hadoop200:9200/_cat/nodes?v" target="_blank" rel="noopener">http://hadoop200:9200/_cat/nodes?v</a></p>
<img src="/2020/01/19/Elasticsearch%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/06.png" class="" title="This is an image">
<p>&emsp;或者在浏览器中输入地址: <a href="http://hadoop200:9200/_cat/nodes?v" target="_blank" rel="noopener">http://hadoop200:9200/_cat/nodes?v</a></p>
<h2 id="问题及解决方案"><a href="#问题及解决方案" class="headerlink" title="问题及解决方案"></a>问题及解决方案</h2><p>&emsp;默认 elasticsearch 是单机访问模式，就是只能自己访问自己。 但是我们之后一定会设置成允许应用服务器通过网络方式访问。这时，elasticsearch 就会因为嫌弃单机版的低端默认配置而报错，甚至无法启动。 所以我们在这里就要把服务器的一些限制打开，能支持更多并发。<br>注意: 修改以下配置的时候需要切换到 root 用户, 每个节点都要修改.<br><strong>问题1：</strong>max file descriptors [4096] for elasticsearch process likely too low, increase to at least [65536] elasticsearch<br>原因：系统允许 Elasticsearch 打开的最大文件数需要修改成 65536<br>解决：<br>&emsp;vim /etc/security/limits.conf 添加内容：<br>&emsp;* soft nofile 65536<br>&emsp;* hard nofile 131072<br>&emsp;* soft nproc 2048<br>&emsp;* hard nproc 65536<br><strong>注意：“*” 不要省略掉</strong><br><strong>问题2：</strong>max number of threads [1024] for user [judy2] likely too low, increase to at least [2048] （CentOS7.x 不用改）<br>原因：允许最大进程数修该成4096<br>解决：<br>&emsp;vim /etc/security/limits.d/90-nproc.conf<br>&emsp;修改如下内容： * soft nproc 1024<br>&emsp;修改为* soft nproc 4096<br><strong>问题3：</strong>max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144] （CentOS7.x 不用改）<br>原因：一个进程可以拥有的虚拟内存区域的数量。<br>解决： 在 /etc/sysctl.conf 文件最后添加一行<br>&emsp;vm.max_map_count=262144</p>
]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
  </entry>
</search>
